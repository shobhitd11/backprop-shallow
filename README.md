# backprop-shallow

Neural Network that classifies MNIST data into 10 classes.
This current Model has backpropogation coded from scratch. Implements 2 versions: Batch SGD and Vanilla GD and consists of 3 layers, Input, RelU hidden and a sofmax output. All parameters of this layer are configurable. All weights are initialized using He and Xavier initialization.

TODO: 

1. Make the number of layers customizable
2. Different activation functions
